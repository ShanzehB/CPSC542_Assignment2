# -*- coding: utf-8 -*-
"""PetDS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pTLgHC5FlR-pBaUJ34T_NuauXfmoH3QS
"""

!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
!
!curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz
!curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz
!
!tar -xf images.tar.gz
!tar -xf annotations.tar.gz

import os

input_dir = "images/"
target_dir = "annotations/trimaps/"
img_size = (160, 160)
num_classes = 3
batch_size = 32

input_img_paths = sorted(
    [
        os.path.join(input_dir, fname)
        for fname in os.listdir(input_dir)
        if fname.endswith(".jpg")
    ]
)
target_img_paths = sorted(
    [
        os.path.join(target_dir, fname)
        for fname in os.listdir(target_dir)
        if fname.endswith(".png") and not fname.startswith(".")
    ]
)

print("Number of samples:", len(input_img_paths))

for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):
    print(input_path, "|", target_path)

from IPython.display import Image, display
from keras.utils import load_img
from PIL import ImageOps

# Display input image
display(Image(filename=input_img_paths[9]))

# Display auto-contrast version of corresponding target (per-pixel categories)
img = ImageOps.autocontrast(load_img(target_img_paths[9]))
display(img)

import os
from PIL import Image  # Add this import statement
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

input_dir = "images/"
target_dir = "annotations/trimaps/"
img_size = (160, 160)
num_classes = 3
batch_size = 32

X_train = input_img_paths  # Assign input_img_paths to X_train

# Example: using the first 500 images as training data
X_train = input_img_paths[:500]
y_train = target_img_paths[:500]

# target_size = (160, 160)


# Define data augmentation parameters
datagen = ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)


print("Number of samples:", len(input_img_paths))

for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):
    print(input_path, "|", target_path)

# Create generator for image and mask augmentation
def image_mask_generator(input_img_paths, target_img_paths, batch_size):
    while True:
        for i in range(0, len(input_img_paths), batch_size):
            batch_input_img_paths = input_img_paths[i:i+batch_size]
            batch_target_img_paths = target_img_paths[i:i+batch_size]

            batch_input_imgs = []
            batch_target_imgs = []
            for input_path, target_path in zip(batch_input_img_paths, batch_target_img_paths):
                # Load and preprocess input image
                input_img = np.array(Image.open(input_path).resize(img_size)) / 255.0
                batch_input_imgs.append(input_img)

                # Load and preprocess target image
                target_img = np.array(Image.open(target_path).resize(img_size)) - 1
                batch_target_imgs.append(target_img)

            yield (np.array(batch_input_imgs), np.array(batch_target_imgs))

# Example usage of generator
train_generator = image_mask_generator(input_img_paths, target_img_paths, batch_size)
input_images, target_masks = next(train_generator)
print("Input images shape:", input_images.shape)
print("Target masks shape:", target_masks.shape)

import os
from PIL import Image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np

input_dir = "images/"
target_dir = "annotations/trimaps/"
img_size = (160, 160)
num_classes = 3
batch_size = 32

# Define data augmentation parameters
datagen = ImageDataGenerator(
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

input_img_paths = sorted(
    [
        os.path.join(input_dir, fname)
        for fname in os.listdir(input_dir)
        if fname.endswith(".jpg")
    ]
)
target_img_paths = sorted(
    [
        os.path.join(target_dir, fname)
        for fname in os.listdir(target_dir)
        if fname.endswith(".png") and not fname.startswith(".")
    ]
)

print("Number of samples:", len(input_img_paths))

# Create generator for image and mask augmentation
def image_mask_generator(input_img_paths, target_img_paths, batch_size):
    while True:
        for i in range(0, len(input_img_paths), batch_size):
            batch_input_img_paths = input_img_paths[i:i+batch_size]
            batch_target_img_paths = target_img_paths[i:i+batch_size]

            batch_input_imgs = []
            batch_target_masks = []
            for input_path, target_path in zip(batch_input_img_paths, batch_target_img_paths):
                # Load and preprocess input image
                input_img = np.array(Image.open(input_path).resize(img_size)) / 255.0
                batch_input_imgs.append(input_img)

                # Load and preprocess target mask
                target_mask = np.array(Image.open(target_path).resize(img_size))
                batch_target_masks.append(target_mask)

            yield (np.array(batch_input_imgs), np.array(batch_target_masks))

# Initialize Random Forest classifier
clf = RandomForestClassifier(n_estimators=1, random_state=42)

# Extract features from the images and masks
train_generator = image_mask_generator(input_img_paths, target_img_paths, batch_size)
for inputs, target_masks in train_generator:
    input_features = np.reshape(inputs, (inputs.shape[0], -1))
    target_labels = np.reshape(target_masks, (target_masks.shape[0], -1))
    # Train Random Forest classifier
    clf.fit(input_features, target_labels)
    break  # Training only on the first batch for demonstration purposes

# Example of predicting on the same batch of data
predictions = clf.predict(input_features)
predictions = np.reshape(predictions, target_masks.shape)

# Evaluate accuracy (just for demonstration, actual evaluation should be done on a separate validation set)
accuracy = accuracy_score(target_masks.flatten(), predictions.flatten())
print("Training Accuracy:", accuracy)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Get true labels (ground truth)
true_labels = target_masks.flatten()

# Predict on the same batch of data
predicted_labels = predictions.flatten()

# Compute confusion matrix
cm = confusion_matrix(true_labels, predicted_labels)

# Plot confusion matrix as heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# Split target_img_paths into training and validation sets
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(input_img_paths, target_img_paths, test_size=0.2, random_state=42)

# Define data generators for training and validation sets
train_generator = image_mask_generator(X_train, y_train, batch_size)
val_generator = image_mask_generator(X_val, y_val, batch_size)


def image_mask_generator(input_img_paths, target_img_paths, batch_size):
    while True:
        for i in range(0, len(input_img_paths), batch_size):
            batch_input_img_paths = input_img_paths[i:i+batch_size]
            batch_target_img_paths = target_img_paths[i:i+batch_size]

            batch_input_imgs = []
            batch_target_masks = []
            for input_path, target_path in zip(batch_input_img_paths, batch_target_img_paths):
                # Load and preprocess input image
                input_img = np.array(Image.open(input_path).resize(img_size)) / 255.0
                batch_input_imgs.append(input_img)

                # Load and preprocess target mask
                target_mask = np.array(Image.open(target_path).resize(img_size))
                batch_target_masks.append(target_mask)

            # Convert lists to numpy arrays
            batch_input_imgs = np.array(batch_input_imgs)
            batch_target_masks = np.array(batch_target_masks)

            # Print shapes and data types
            print("Batch Input Images Shape:", batch_input_imgs.shape)
            print("Batch Input Images Data Type:", batch_input_imgs.dtype)
            print("Batch Target Masks Shape:", batch_target_masks.shape)
            print("Batch Target Masks Data Type:", batch_target_masks.dtype)

            yield (batch_input_imgs, batch_target_masks)

# Define data generators for training and validation sets
train_generator = image_mask_generator(X_train, y_train, batch_size)
val_generator = image_mask_generator(X_val, y_val, batch_size)

# Example usage of generator
input_images, target_masks = next(train_generator)
print("Input images shape:", input_images.shape)
print("Target masks shape:", target_masks.shape)

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers

def UNet(input_shape=(160, 160, 3)):
    inputs = Input(input_shape)

    # Downsample path with increasing dropout rate
    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    pool1 = Dropout(0.25)(pool1)

    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    pool2 = Dropout(0.3)(pool2)

    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    pool3 = Dropout(0.35)(pool3)

    # Bottleneck without dropout
    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv4)

    # Upsample path with decreasing dropout rate
    up5 = UpSampling2D(size=(2, 2))(conv4)
    up5 = Conv2D(256, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(up5)
    merge5 = concatenate([conv3, up5], axis=3)
    conv5 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(merge5)
    conv5 = Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv5)
    conv5 = Dropout(0.35)(conv5)

    up6 = UpSampling2D(size=(2, 2))(conv5)
    up6 = Conv2D(128, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(up6)
    merge6 = concatenate([conv2, up6], axis=3)
    conv6 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(merge6)
    conv6 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv6)
    conv6 = Dropout(0.3)(conv6)

    up7 = UpSampling2D(size=(2, 2))(conv6)
    up7 = Conv2D(64, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(up7)
    merge7 = concatenate([conv1, up7], axis=3)
    conv7 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(merge7)
    conv7 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(conv7)
    conv7 = Dropout(0.25)(conv7)

    # Output layer
    outputs = Conv2D(3, 1, activation='softmax')(conv7)  # 3 classes for segmentation

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Create UNet model
model = UNet()

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display model summary
model.summary()


import numpy as np
from sklearn.model_selection import train_test_split
from PIL import Image


# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(input_img_paths, target_img_paths, test_size=0.2, random_state=42)

# Define the target size for resizing
target_size = (160, 160)

def load_and_preprocess_data(img_paths):
    data = []
    for img_path in img_paths:
        img = Image.open(img_path).convert('RGB').resize(target_size)
        data.append(np.array(img))
    return np.array(data)

# Load and preprocess the training and validation data
X_train_data = load_and_preprocess_data(X_train)
y_train_data = load_and_preprocess_data(y_train)
X_val_data = load_and_preprocess_data(X_val)
y_val_data = load_and_preprocess_data(y_val)

# Normalize the data
X_train_data = X_train_data / 255.0
y_train_data = y_train_data / 255.0
X_val_data = X_val_data / 255.0
y_val_data = y_val_data / 255.0

# Check data consistency
assert len(X_train_data) == len(y_train_data), "Training input and target data have different lengths"
assert len(X_val_data) == len(y_val_data), "Validation input and target data have different lengths"

# Print the number of samples in each dataset
print("Number of training samples:", len(X_train_data))
print("Number of validation samples:", len(X_val_data))

epochs = 10
batch_size = 32

from tensorflow.keras.optimizers import Adam

# Define the optimizer
optimizer = Adam(learning_rate=0.001)

# Compile the model with the defined optimizer
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])


history = model.fit(X_train_data, y_train_data,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_val_data, y_val_data))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Function to plot accuracy graph
def plot_accuracy(history):
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

plot_accuracy(history)